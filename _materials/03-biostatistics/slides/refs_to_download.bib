@UNPUBLISHED{Parmigiani2023-dr,
  title = {Defining replicability of prediction rules},
  author = {Parmigiani, Giovanni},
  journaltitle = {arXiv [stat.ME]},
  date = {2023-04-30},
  eprint = {2305.01518},
  eprintclass = {stat.ME},
  abstract = {In this article I propose an approach for defining replicability
  for prediction rules. Motivated by a recent NAS report, I start from the
  perspective that replicability is obtaining consistent results across studies
  suitable to address the same prediction question, each of which has obtained
  its own data. I then discuss concept and issues in defining key elements of
  this statement. I focus specifically on the meaning of "consistent results" in
  typical utilization contexts, and propose a multi-agent framework for defining
  replicability, in which agents are neither partners nor adversaries. I recover
  some of the prevalent practical approaches as special cases. I hope to provide
  guidance for a more systematic assessment of replicability in machine
  learning.},
  url = {http://arxiv.org/abs/2305.01518},
  urldate = {2023-08-22}
}

@ARTICLE{Brenner1997-yq,
  title = {Variation of sensitivity, specificity, likelihood ratios and
  predictive values with disease prevalence},
  author = {Brenner, H and Gefeller, O},
  journaltitle = {Stat. Med.},
  publisher = {Wiley},
  volume = {16},
  issue = {9},
  pages = {981-991},
  date = {1997-05-15},
  doi = {10.1002/(sici)1097-0258(19970515)16:9<981::aid-sim510>3.0.co;2-n},
  pmid = {9160493},
  issn = {1097-0258,0277-6715},
  abstract = {The sensitivity, specificity and likelihood ratios of binary
  diagnostic tests are often thought of as being independent of disease
  prevalence. Empirical studies, however, have frequently revealed substantial
  variation of these measures for the same diagnostic test in different
  populations. One reason for this discrepancy is related to the fact that only
  few diagnostic tests are inherently dichotomous. The majority of tests are
  based on categorization of individuals according to one or several underlying
  continuous traits. For these tests, the magnitude of diagnostic
  misclassification depends not only on the magnitude of the measurement or
  perception error of the underlying trait(s), but also on the distribution of
  the underlying trait(s) in the population relative to the diagnostic cutpoint.
  Since this distribution also determines prevalence of the disease in the
  population, diagnostic misclassification and disease prevalence are related
  for this type of test. We assess the variation of various measures of validity
  of diagnostic tests with disease prevalence for simple models of the
  distribution of the underlying trait(s) and the measurement or perception
  error. We illustrate that variation with disease prevalence is typically
  strong for sensitivity and specificity, and even more so for the likelihood
  ratios. Although positive and negative predictive values also strongly vary
  with disease prevalence, this variation is usually less pronounced than one
  would expect if sensitivity and specificity were independent of disease
  prevalence.},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/%28SICI%291097-0258%2819970515%2916%3A9%3C981%3A%3AAID-SIM510%3E3.0.CO%3B2-N},
  urldate = {2023-08-18},
  language = {en}
}

@ARTICLE{Carpenter2000-dm,
  title = {Bootstrap confidence intervals: when, which, what? A practical guide
  for medical statisticians},
  author = {Carpenter, J and Bithell, J},
  journaltitle = {Stat. Med.},
  publisher = {Wiley},
  volume = {19},
  issue = {9},
  pages = {1141-1164},
  date = {2000-05-15},
  doi = {10.1002/(sici)1097-0258(20000515)19:9<1141::aid-sim479>3.0.co;2-f},
  pmid = {10797513},
  issn = {1097-0258,0277-6715},
  abstract = {Since the early 1980s, a bewildering array of methods for
  constructing bootstrap confidence intervals have been proposed. In this
  article, we address the following questions. First, when should bootstrap
  confidence intervals be used. Secondly, which method should be chosen, and
  thirdly, how should it be implemented. In order to do this, we review the
  common algorithms for resampling and methods for constructing bootstrap
  confidence intervals, together with some less well known ones, highlighting
  their strengths and weaknesses. We then present a simulation study, a flow
  chart for choosing an appropriate method and a survival analysis example.},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/%28SICI%291097-0258%2820000515%2919%3A9%3C1141%3A%3AAID-SIM479%3E3.0.CO%3B2-F},
  urldate = {2023-08-23},
  language = {en}
}

@ARTICLE{Patil2018-hm,
  title = {Training replicable predictors in multiple studies},
  author = {Patil, Prasad and Parmigiani, Giovanni},
  journaltitle = {Proc. Natl. Acad. Sci. U. S. A.},
  publisher = {National Academy of Sciences},
  volume = {115},
  issue = {11},
  pages = {2578-2583},
  date = {2018-03-13},
  doi = {10.1073/pnas.1708283115},
  pmc = {PMC5856504},
  pmid = {29531060},
  issn = {0027-8424,1091-6490},
  abstract = {This article considers replicability of the performance of
  predictors across studies. We suggest a general approach to investigating this
  issue, based on ensembles of prediction models trained on different studies.
  We quantify how the common practice of training on a single study accounts in
  part for the observed challenges in replicability of prediction performance.
  We also investigate whether ensembles of predictors trained on multiple
  studies can be combined, using unique criteria, to design robust ensemble
  learners trained upfront to incorporate replicability into different contexts
  and populations.},
  url = {https://www.pnas.org/doi/abs/10.1073/pnas.1708283115},
  urldate = {2023-08-22},
  keywords = {cross-study validation; ensemble learning; machine learning;
  replicability; validation},
  language = {en}
}

@ARTICLE{Ganzfried2013-ut,
  title = {curatedOvarianData: clinically annotated data for the ovarian cancer
  transcriptome},
  author = {Ganzfried, Benjamin Frederick and Riester, Markus and Haibe-Kains,
  Benjamin and Risch, Thomas and Tyekucheva, Svitlana and Jazic, Ina and Wang,
  Xin Victoria and Ahmadifar, Mahnaz and Birrer, Michael J and Parmigiani,
  Giovanni and Huttenhower, Curtis and Waldron, Levi},
  journaltitle = {Database (Oxford)},
  publisher = {Oxford University Press (OUP)},
  volume = {2013},
  pages = {bat013},
  date = {2013-04-02},
  doi = {10.1093/database/bat013},
  pmc = {PMC3625954},
  pmid = {23550061},
  issn = {1758-0463},
  abstract = {This article introduces a manually curated data collection for
  gene expression meta-analysis of patients with ovarian cancer and software for
  reproducible preparation of similar databases. This resource provides
  uniformly prepared microarray data for 2970 patients from 23 studies with
  curated and documented clinical metadata. It allows users to efficiently
  identify studies and patient subgroups of interest for analysis and to perform
  meta-analysis immediately without the challenges posed by harmonizing
  heterogeneous microarray technologies, study designs, expression data
  processing methods and clinical data formats. We confirm that the recently
  proposed biomarker CXCL12 is associated with patient survival, independently
  of stage and optimal surgical debulking, which was possible only through
  meta-analysis owing to insufficient sample sizes of the individual studies.
  The database is implemented as the curatedOvarianData Bioconductor package for
  the R statistical computing language, providing a comprehensive and flexible
  resource for clinically oriented investigation of the ovarian cancer
  transcriptome. The package and pipeline for producing it are available from
  http://bcb.dfci.harvard.edu/ovariancancer.},
  url = {https://academic.oup.com/database/article/doi/10.1093/database/bat013/330978?view=long},
  language = {en}
}

@ARTICLE{Masoero2023-wb,
  title = {Cross-Study Replicability in Cluster Analysis},
  author = {Masoero, Lorenzo and Thomas, Emma and Parmigiani, Giovanni and
  Tyekucheva, Svitlana and Trippa, Lorenzo},
  journaltitle = {Stat. Sci.},
  publisher = {Institute of Mathematical Statistics},
  volume = {38},
  issue = {2},
  pages = {303-316},
  date = {2023-05-01},
  doi = {10.1214/22-sts871},
  issn = {0883-4237,2168-8745},
  abstract = {In cancer research, clustering techniques are widely used for
  exploratory analyses, playing a critical role in the identification of novel
  cancer subtypes and patient management. As data collected by multiple research
  groups grows, it is increasingly feasible to investigate the replicability of
  clustering procedures, that is, their ability to consistently recover
  biologically meaningful clusters across several data sets. In this paper, we
  review methods for replicability of clustering analyses, and discuss a novel
  framework for evaluating cross-study clustering replicability, useful when two
  or more studies are available. Our approach can be applied to any clustering
  algorithm and can employ different measures of similarity between partitions
  to quantify replicability, globally (i.e., for the whole sample) as well as
  locally (i.e., for individual clusters). Using experiments on synthetic and
  real gene expression data, we illustrate the usefulness of our procedure to
  evaluate if the same clusters are identified consistently across a collection
  of data sets.},
  url = {https://projecteuclid.org/journals/statistical-science/volume-38/issue-2/Cross-Study-Replicability-in-Cluster-Analysis/10.1214/22-STS871.short},
  urldate = {2023-08-22},
  keywords = {clustering; multiple studies; replicability},
  language = {en}
}

