@INBOOK{Schauer2022-mj,
  title = {Replicability and Meta-Analysis},
  author = {Schauer, Jacob M},
  editor = {O'Donohue, William and Masuda, Akihiko and Lilienfeld, Scott},
  booktitle = {Avoiding Questionable Research Practices in Applied Psychology},
  publisher = {Springer International Publishing},
  location = {Cham},
  pages = {301-342},
  date = {2022},
  doi = {10.1007/978-3-031-04968-2_14},
  isbn = {9783031049682,9783031049682},
  abstract = {In this chapter, I will discuss statistical considerations for
  studying replication. More specifically, I will approach replication from a
  framework based on meta-analysis. To do so, I will focus on direct
  replications, where studies are designed to be as similar as possible, as
  opposed to conceptual replications that (systematically or haphazardly) vary
  in at least one aspect of an experiment. The chapter starts with a brief
  description of recent research on replication in psychology and uses examples
  from that research to highlight relevant considerations in defining and
  parametrizing “replication.” It then outlines different ways to frame analyses
  of replication and provides examples. Finally, it takes one possible
  definition of replication—that effects found across studies involving the same
  phenomenon are consistent—and describes relevant analyses and their
  properties.},
  url = {https://doi.org/10.1007/978-3-031-04968-2_14},
  urldate = {2023-09-01},
  keywords = {replication-methods},
  language = {en}
}

@ARTICLE{Valentine2011-yq,
  title = {Replication in prevention science},
  author = {Valentine, Jeffrey C and Biglan, Anthony and Boruch, Robert F and
  Castro, Felipe González and Collins, Linda M and Flay, Brian R and Kellam,
  Sheppard and Mościcki, Eve K and Schinke, Steven P},
  journaltitle = {Prev. Sci.},
  publisher = {Springer Science and Business Media LLC},
  volume = {12},
  issue = {2},
  pages = {103-117},
  date = {2011-06-04},
  doi = {10.1007/s11121-011-0217-6},
  pmid = {21541692},
  issn = {1389-4986,1573-6695},
  abstract = {Replication research is essential for the advancement of any
  scientific field. In this paper, we argue that prevention science will be
  better positioned to help improve public health if (a) more replications are
  conducted; (b) those replications are systematic, thoughtful, and conducted
  with full knowledge of the trials that have preceded them; and (c)
  state-of-the art techniques are used to summarize the body of evidence on the
  effects of the interventions. Under real-world demands it is often not
  feasible to wait for multiple replications to accumulate before making
  decisions about intervention adoption. To help individuals and agencies make
  better decisions about intervention utility, we outline strategies that can be
  used to help understand the likely direction, size, and range of intervention
  effects as suggested by the current knowledge base. We also suggest structural
  changes that could increase the amount and quality of replication research,
  such as the provision of incentives and a more vigorous pursuit of prospective
  research registers. Finally, we discuss methods for integrating replications
  into the roll-out of a program and suggest that strong partnerships with local
  decision makers are a key component of success in replication research. Our
  hope is that this paper can highlight the importance of replication and
  stimulate more discussion of the important elements of the replication
  process. We are confident that, armed with more and better replications and
  state-of-the-art review methods, prevention science will be in a better
  position to positively impact public health.},
  url = {https://link.springer.com/article/10.1007/s11121-011-0217-6},
  urldate = {2023-07-31},
  keywords = {replication-methods},
  language = {en}
}

@ARTICLE{Wagenmakers2010-fj,
  title = {Bayesian hypothesis testing for psychologists: A tutorial on the
  Savage–Dickey method},
  author = {Wagenmakers, Eric-Jan and Lodewyckx, Tom and Kuriyal, Himanshu and
  Grasman, Raoul},
  journaltitle = {Cogn. Psychol.},
  volume = {60},
  issue = {3},
  pages = {158-189},
  date = {2010-05-01},
  doi = {10.1016/j.cogpsych.2009.12.001},
  issn = {0010-0285},
  abstract = {In the field of cognitive psychology, the p-value hypothesis test
  has established a stranglehold on statistical reporting. This is unfortunate,
  as the p-value provides at best a rough estimate of the evidence that the data
  provide for the presence of an experimental effect. An alternative and
  arguably more appropriate measure of evidence is conveyed by a Bayesian
  hypothesis test, which prefers the model with the highest average likelihood.
  One of the main problems with this Bayesian hypothesis test, however, is that
  it often requires relatively sophisticated numerical methods for its
  computation. Here we draw attention to the Savage–Dickey density ratio method,
  a method that can be used to compute the result of a Bayesian hypothesis test
  for nested models and under certain plausible restrictions on the parameter
  priors. Practical examples demonstrate the method’s validity, generality, and
  flexibility.},
  url = {http://www.sciencedirect.com/science/article/pii/S0010028509000826},
  keywords = {Statistical evidence; Model selection; Bayes factor; Hierarchical
  modeling; Random effects; Order-restrictions;Bayesian Statistics}
}

@ARTICLE{Hedges1980-gd,
  title = {Vote-counting methods in research synthesis},
  author = {Hedges, Larry V and Olkin, Ingram},
  journaltitle = {Psychol. Bull.},
  publisher = {American Psychological Association (APA)},
  volume = {88},
  issue = {2},
  pages = {359-369},
  date = {1980-09},
  doi = {10.1037/0033-2909.88.2.359},
  issn = {0033-2909,1939-1455},
  url = {https://psycnet.apa.org/record/1980-29312-001},
  language = {en}
}

@ARTICLE{Schmidt2009-mq,
  title = {Shall we Really do it Again? The Powerful Concept of Replication is
  Neglected in the Social Sciences},
  author = {Schmidt, Stefan},
  journaltitle = {Rev. Gen. Psychol.},
  publisher = {SAGE Publications},
  volume = {13},
  issue = {2},
  pages = {90-100},
  date = {2009-06},
  doi = {10.1037/a0015108},
  issn = {1089-2680,1939-1552},
  abstract = {Replication is one of the most important tools for the
  verification of facts within the empirical sciences. A detailed examination of
  the notion of replication reveals that there are many different meanings to
  this concept and the relevant procedures, but hardly any systematic
  literature. This paper analyzes the concept of replication from a theoretical
  point of view. It demonstrates that the theoretical demands are scarcely met
  in everyday work within the social sciences. Some demands are just not
  feasible, whereas others are constricted by restrictions relating to
  publication. A new classification scheme based on a functional approach that
  distinguishes between different types of replication is proposed. Next, it
  will be argued that replication addresses the important connection between
  existing and new knowledge. To do so it has to be applied explicitly and
  systematically. The paper ends with a description of procedures how this could
  be done and a set of recommendations how to handle the concept of replication
  in the future to exploit its potential to the full.},
  url = {https://journals.sagepub.com/doi/abs/10.1037/a0015108},
  language = {en}
}

@ARTICLE{Schauer2020-tw,
  title = {Assessing heterogeneity and power in replications of psychological
  experiments},
  author = {Schauer, Jacob M and Hedges, Larry V},
  journaltitle = {Psychol. Bull.},
  publisher = {American Psychological Association (APA)},
  volume = {146},
  issue = {8},
  pages = {701-719},
  date = {2020-08},
  doi = {10.1037/bul0000232},
  pmid = {32271029},
  issn = {0033-2909,1939-1455},
  abstract = {In this study, we reanalyze recent empirical research on
  replication from a meta-analytic perspective. We argue that there are
  different ways to define "replication failure," and that analyses can focus on
  exploring variation among replication studies or assess whether their results
  contradict the findings of the original study. We apply this framework to a
  set of psychological findings that have been replicated and assess the
  sensitivity of these analyses. We find that tests for replication that involve
  only a single replication study are almost always severely underpowered. Among
  the 40 findings for which ensembles of multisite direct replications were
  conducted, we find that between 11 and 17 (28\% to 43\%) ensembles produced
  heterogeneous effects, depending on how replication is defined. This
  heterogeneity could not be completely explained by moderators documented by
  replication research programs. We also find that these ensembles were not
  always well-powered to detect potentially meaningful values of heterogeneity.
  Finally, we identify several discrepancies between the results of original
  studies and the distribution of effects found by multisite replications but
  note that these analyses also have low power. We conclude by arguing that
  efforts to assess replication would benefit from further methodological work
  on designing replication studies to ensure analyses are sufficiently
  sensitive. (PsycInfo Database Record (c) 2020 APA, all rights reserved).},
  url = {http://dx.doi.org/10.1037/bul0000232},
  urldate = {2023-07-31},
  keywords = {replication-methods},
  language = {en}
}

@ARTICLE{Hedges2019-ry,
  title = {Statistical analyses for studying replication: Meta-analytic
  perspectives},
  author = {Hedges, Larry V and Schauer, Jacob M},
  journaltitle = {Psychol. Methods},
  publisher = {American Psychological Association (APA)},
  volume = {24},
  issue = {5},
  pages = {557-570},
  date = {2019-10},
  doi = {10.1037/met0000189},
  pmid = {30070547},
  issn = {1082-989X,1939-1463},
  abstract = {Formal empirical assessments of replication have recently become
  more prominent in several areas of science, including psychology. These
  assessments have used different statistical approaches to determine if a
  finding has been replicated. The purpose of this article is to provide several
  alternative conceptual frameworks that lead to different statistical analyses
  to test hypotheses about replication. All of these analyses are based on
  statistical methods used in meta-analysis. The differences among the methods
  described involve whether the burden of proof is placed on replication or
  nonreplication, whether replication is exact or allows for a small amount of
  "negligible heterogeneity," and whether the studies observed are assumed to be
  fixed (constituting the entire body of relevant evidence) or are a sample from
  a universe of possibly relevant studies. The statistical power of each of
  these tests is computed and shown to be low in many cases, raising issues of
  the interpretability of tests for replication. (PsycINFO Database Record (c)
  2019 APA, all rights reserved).},
  url = {http://dx.doi.org/10.1037/met0000189},
  file = {Hedges and Schauer 2019 - Statistical analyses for studying replication - Meta-analytic perspectives.pdf},
  keywords = {replication-methods},
  language = {en}
}

@ARTICLE{Mathur2019-vh,
  title = {Challenges and suggestions for defining replication "success" when
  effects may be heterogeneous: Comment on Hedges and Schauer (2019)},
  author = {Mathur, Maya B and VanderWeele, Tyler J},
  journaltitle = {Psychol. Methods},
  publisher = {American Psychological Association (APA)},
  volume = {24},
  issue = {5},
  pages = {571-575},
  date = {2019-10},
  doi = {10.1037/met0000223},
  pmc = {PMC6779319},
  pmid = {31580141},
  issn = {1082-989X,1939-1463},
  abstract = {Psychological scientists are now trying to replicate published
  research from scratch to confirm the findings. In an increasingly widespread
  replication study design, each of several collaborating sites (such as
  universities) independently tries to replicate an original study, and the
  results are synthesized across sites. Hedges and Schauer (2019) proposed
  statistical analyses for these replication projects; their analyses focus on
  assessing the extent to which results differ across the replication sites, by
  testing for heterogeneity among a set of replication studies, while excluding
  the original study. We agree with their premises regarding the limitations of
  existing analysis methods and regarding the importance of accounting for
  heterogeneity among the replications. This objective may be interesting in its
  own right. However, we argue that by focusing only on whether the replication
  studies have similar effect sizes to one another, these analyses are not
  particularly appropriate for assessing whether the replications in fact
  support the scientific effect under investigation or for assessing the power
  of multisite replication projects. We reanalyze Hedges and Schauer's (2019)
  example dataset using alternative metrics of replication success that directly
  address these objectives. We reach a more optimistic conclusion regarding
  replication success than they did, illustrating that the alternative metrics
  can lead to quite different conclusions from those of Hedges and Schauer
  (2019). (PsycINFO Database Record (c) 2019 APA, all rights reserved).},
  url = {http://dx.doi.org/10.1037/met0000223},
  urldate = {2023-07-28},
  keywords = {replication-methods},
  language = {en}
}

@ARTICLE{Hedges2019-pr,
  title = {Consistency of effects is important in replication: Rejoinder to
  Mathur and VanderWeele (2019)},
  author = {Hedges, Larry V and Schauer, Jacob M},
  journaltitle = {Psychol. Methods},
  publisher = {American Psychological Association (APA)},
  volume = {24},
  issue = {5},
  pages = {576-577},
  date = {2019-10},
  doi = {10.1037/met0000237},
  pmid = {31580142},
  issn = {1082-989X,1939-1463},
  abstract = {In this rejoinder, we discuss Mathur and VanderWeele's response to
  our article, "Statistical Analyses for Studying Replication: Meta-Analytic
  Perspectives," which appears in this current issue. We attempt to clarify a
  point of confusion regarding the inclusion of an original study in an analysis
  of replication, and the potential impact of publication bias. We then discuss
  the methods used by Mathur and VanderWeele to conduct an alternative analysis
  of the Gambler's Fallacy example from our article. We highlight that there are
  some potential statistical and conceptual differences to their approach
  compared to what we propose in our article. (PsycINFO Database Record (c) 2019
  APA, all rights reserved).},
  url = {http://dx.doi.org/10.1037/met0000237},
  urldate = {2023-09-01},
  keywords = {replication-methods},
  language = {en}
}

@ARTICLE{Schauer2021-ja,
  title = {Reconsidering statistical methods for assessing replication},
  author = {Schauer, J M and Hedges, L V},
  journaltitle = {Psychol. Methods},
  publisher = {American Psychological Association (APA)},
  volume = {26},
  issue = {1},
  pages = {127-139},
  date = {2021-02},
  doi = {10.1037/met0000302},
  pmid = {33617275},
  issn = {1082-989X,1939-1463},
  abstract = {Recent empirical evaluations of replication in psychology have
  reported startlingly few successful replication attempts. At the same time,
  these programs have noted that the proper way to analyze replication studies
  is far from a settled matter and have analyzed their data in several different
  ways. This presents 2 challenges to interpreting the results of these
  programs. First, different analysis methods assess different operational
  definitions of replication. Second, the properties of these methods are not
  necessarily common knowledge; it is possible for a successful replication to
  be deemed a failure by nearly all of the metrics used, and it is not always
  immediately clear how likely such errors are to occur. In this article, we
  describe the methods commonly used in replication research and how they imply
  specific operational definitions of replication. We then compute the
  probability of false failure (i.e., a successful replication is concluded to
  have failed) and false success determinations. These are shown to be high
  (often over 50\%) and in many cases uncontrolled. We then demonstrate that
  errors are probable in the data to which these methods have been applied in
  the literature. We show that the probability that some reported conclusions
  about replication are incorrect can be as high as 75-80\%. (PsycInfo Database
  Record (c) 2021 APA, all rights reserved).},
  url = {http://dx.doi.org/10.1037/met0000302},
  urldate = {2023-07-31},
  keywords = {replication-methods},
  language = {en}
}

@ARTICLE{Baumeister2007-jb,
  title = {The strength model of self-control},
  author = {Baumeister, Roy F and Vohs, Kathleen D and Tice, Dianne M},
  journaltitle = {Curr. Dir. Psychol. Sci.},
  publisher = {SAGE Publications},
  volume = {16},
  issue = {6},
  pages = {351-355},
  date = {2007-12},
  doi = {10.1111/j.1467-8721.2007.00534.x},
  issn = {0963-7214,1467-8721},
  abstract = {Self-control is a central function of the self and an important
  key to success in life. The exertion of self-control appears to depend on a
  limited resource. Just as a muscle gets tired from exertion, acts of
  self-control cause short-term impairments (ego depletion) in subsequent
  self-control, even on unrelated tasks. Research has supported the strength
  model in the domains of eating, drinking, spending, sexuality, intelligent
  thought, making choices, and interpersonal behavior. Motivational or framing
  factors can temporarily block the deleterious effects of being in a state of
  ego depletion. Blood glucose is an important component of the energy.},
  url = {https://journals.sagepub.com/doi/abs/10.1111/j.1467-8721.2007.00534.x},
  language = {en}
}

@ARTICLE{Mathur2020-nw,
  title = {New statistical metrics for multisite replication projects},
  author = {Mathur, Maya B and VanderWeele, Tyler J},
  journaltitle = {J. R. Stat. Soc. Ser. A Stat. Soc.},
  publisher = {Oxford University Press (OUP)},
  volume = {183},
  issue = {3},
  pages = {1145-1166},
  date = {2020-06-01},
  doi = {10.1111/rssa.12572},
  issn = {0964-1998,1467-985X},
  abstract = {Summary Increasingly, researchers are attempting to replicate
  published original studies by using large, multisite replication projects, at
  least 134 of which have been completed or are on going. These designs are
  promising to assess whether the original study is statistically consistent
  with the replications and to reassess the strength of evidence for the
  scientific effect of interest. However, existing analyses generally focus on
  single replications; when applied to multisite designs, they provide an
  incomplete view of aggregate evidence and can lead to misleading conclusions
  about replication success. We propose new statistical metrics representing
  firstly the probability that the original study's point estimate would be at
  least as extreme as it actually was, if in fact the original study were
  statistically consistent with the replications, and secondly the estimated
  proportion of population effects agreeing in direction with the original
  study. Generalized versions of the second metric enable consideration of only
  meaningfully strong population effects that agree in direction, or
  alternatively that disagree in direction, with the original study. These
  metrics apply when there are at least 10 replications (unless the
  heterogeneity estimate τ\textasciicircum=0, in which case the metrics apply
  regardless of the number of replications). The first metric assumes normal
  population effects but appears robust to violations in simulations; the second
  is distribution free. We provide R packages (Replicate and MetaUtility).},
  url = {https://academic.oup.com/jrsssa/article-abstract/183/3/1145/7056432},
  keywords = {replication-methods},
  language = {en}
}

@ARTICLE{Hedges2021-of,
  title = {The design of replication studies},
  author = {Hedges, Larry V and Schauer, Jacob M},
  journaltitle = {J. R. Stat. Soc. Ser. A Stat. Soc.},
  publisher = {Oxford University Press (OUP)},
  volume = {184},
  issue = {3},
  pages = {868-886},
  date = {2021-07-01},
  doi = {10.1111/rssa.12688},
  issn = {0964-1998,1467-985X},
  abstract = {Abstract Empirical evaluations of replication have become
  increasingly common, but there has been no unified approach to doing so. Some
  evaluations conduct only a single replication study while others run several,
  usually across multiple laboratories. Designing such programs has largely
  contended with difficult issues about which experimental components are
  necessary for a set of studies to be considered replications. However, another
  important consideration is that replication studies be designed to support
  sufficiently sensitive analyses. For instance, if hypothesis tests are to be
  conducted about replication, studies should be designed to ensure these tests
  are well-powered; if not, it can be difficult to determine conclusively if
  replication attempts succeeded or failed. This paper describes methods for
  designing ensembles of replication studies to ensure that they are both
  adequately sensitive and cost-efficient. It describes two potential analyses
  of replication studies—hypothesis tests and variance component estimation—and
  approaches to obtaining optimal designs for them. Using these results, it
  assesses the statistical power, precision of point estimators and optimality
  of the design used by the Many Labs Project and finds that while it may have
  been sufficiently powered to detect some larger differences between studies,
  other designs would have been less costly and/or produced more precise
  estimates or higher-powered hypothesis tests.},
  url = {https://academic.oup.com/jrsssa/article-pdf/184/3/868/49411808/jrsssa_184_3_868.pdf},
  urldate = {2023-09-01},
  keywords = {replication-methods},
  language = {en}
}

@ARTICLE{Eskine2011-am,
  title = {A bad taste in the mouth: gustatory disgust influences moral
  judgment: Gustatory disgust influences moral judgment},
  author = {Eskine, Kendall J and Kacinik, Natalie A and Prinz, Jesse J},
  journaltitle = {Psychol. Sci.},
  publisher = {SAGE Publications},
  volume = {22},
  issue = {3},
  pages = {295-299},
  date = {2011-03},
  doi = {10.1177/0956797611398497},
  pmid = {21307274},
  issn = {0956-7976,1467-9280},
  abstract = {Can sweet-tasting substances trigger kind, favorable judgments
  about other people? What about substances that are disgusting and bitter?
  Various studies have linked physical disgust to moral disgust, but despite the
  rich and sometimes striking findings these studies have yielded, no research
  has explored morality in conjunction with taste, which can vary greatly and
  may differentially affect cognition. The research reported here tested the
  effects of taste perception on moral judgments. After consuming a sweet
  beverage, a bitter beverage, or water, participants rated a variety of moral
  transgressions. Results showed that taste perception significantly affected
  moral judgments, such that physical disgust (induced via a bitter taste)
  elicited feelings of moral disgust. Further, this effect was more pronounced
  in participants with politically conservative views than in participants with
  politically liberal views. Taken together, these differential findings suggest
  that embodied gustatory experiences may affect moral processing more than
  previously thought.},
  url = {https://journals.sagepub.com/doi/abs/10.1177/0956797611398497},
  language = {en}
}

@ARTICLE{Simonsohn2015-kg,
  title = {Small telescopes: detectability and the evaluation of replication
  results},
  author = {Simonsohn, Uri},
  journaltitle = {Psychol. Sci.},
  publisher = {journals.sagepub.com},
  volume = {26},
  issue = {5},
  pages = {559-569},
  date = {2015-05},
  doi = {10.1177/0956797614567341},
  pmid = {25800521},
  issn = {0956-7976,1467-9280},
  abstract = {This article introduces a new approach for evaluating replication
  results. It combines effect-size estimation with hypothesis testing, assessing
  the extent to which the replication results are consistent with an effect size
  big enough to have been detectable in the original study. The approach is
  demonstrated by examining replications of three well-known findings. Its
  benefits include the following: (a) differentiating "unsuccessful" replication
  attempts (i.e., studies yielding p > .05) that are too noisy from those that
  actively indicate the effect is undetectably different from zero, (b)
  "protecting" true findings from underpowered replications, and (c) arriving at
  intuitively compelling inferences in general and for the revisited
  replications in particular.},
  url = {https://journals.sagepub.com/doi/abs/10.1177/0956797614567341},
  keywords = {hypothesis testing; open materials; replication; statistical
  power;replication-methods},
  language = {en}
}

@ARTICLE{Patil2016-vc,
  title = {What should researchers expect when they replicate studies? A
  statistical view of replicability in psychological science},
  author = {Patil, Prasad and Peng, Roger D and Leek, Jeffrey T},
  journaltitle = {Perspect. Psychol. Sci.},
  publisher = {SAGE Publications},
  volume = {11},
  issue = {4},
  pages = {539-544},
  date = {2016-07-29},
  doi = {10.1177/1745691616646366},
  pmc = {PMC4968573},
  pmid = {27474140},
  issn = {1745-6916,1745-6924},
  abstract = {A recent study of the replicability of key psychological findings
  is a major contribution toward understanding the human side of the scientific
  process. Despite the careful and nuanced analysis reported, the simple
  narrative disseminated by the mass, social, and scientific media was that in
  only 36\% of the studies were the original results replicated. In the current
  study, however, we showed that 77\% of the replication effect sizes reported
  were within a 95\% prediction interval calculated using the original effect
  size. Our analysis suggests two critical issues in understanding replication
  of psychological studies. First, researchers' intuitive expectations for what
  a replication should show do not always match with statistical estimates of
  replication. Second, when the results of original studies are very imprecise,
  they create wide prediction intervals-and a broad range of replication effects
  that are consistent with the original estimates. This may lead to effects that
  replicate successfully, in that replication results are consistent with
  statistical expectations, but do not provide much information about the size
  (or existence) of the true effect. In this light, the results of the
  Reproducibility Project: Psychology can be viewed as statistically consistent
  with what one might expect when performing a large-scale replication
  experiment.},
  url = {https://journals.sagepub.com/doi/full/10.1177/1745691616646366},
  urldate = {2023-07-31},
  keywords = {Reproducibility Project: Psychology; p values; prediction
  intervals; replication; reproducibility},
  language = {en}
}

@ARTICLE{Hagger2016-vr,
  title = {A multilab preregistered replication of the ego-depletion effect},
  author = {Hagger, Martin S and Chatzisarantis, Nikos L D and Alberts, Hugo and
  Anggono, Calvin Octavianus and Batailler, Cédric and Birt, Angela R and Brand,
  Ralf and Brandt, Mark J and Brewer, Gene and Bruyneel, Sabrina and Calvillo,
  Dustin P and Campbell, W Keith and Cannon, Peter R and Carlucci, Marianna and
  Carruth, Nicholas P and Cheung, Tracy and Crowell, Adrienne and De Ridder,
  Denise T D and Dewitte, Siegfried and Elson, Malte and Evans, Jacqueline R and
  Fay, Benjamin A and Fennis, Bob M and Finley, Anna and Francis, Zoë and Heise,
  Elke and Hoemann, Henrik and Inzlicht, Michael and Koole, Sander L and Koppel,
  Lina and Kroese, Floor and Lange, Florian and Lau, Kevin and Lynch, Bridget P
  and Martijn, Carolien and Merckelbach, Harald and Mills, Nicole V and
  Michirev, Alexej and Miyake, Akira and Mosser, Alexandra E and Muise, Megan
  and Muller, Dominique and Muzi, Milena and Nalis, Dario and Nurwanti, Ratri
  and Otgaar, Henry and Philipp, Michael C and Primoceri, Pierpaolo and
  Rentzsch, Katrin and Ringos, Lara and Schlinkert, Caroline and Schmeichel,
  Brandon J and Schoch, Sarah F and Schrama, Michel and Schütz, Astrid and
  Stamos, Angelos and Tinghög, Gustav and Ullrich, Johannes and vanDellen,
  Michelle and Wimbarti, Supra and Wolff, Wanja and Yusainy, Cleoputri and
  Zerhouni, Oulmann and Zwienenberg, Maria},
  journaltitle = {Perspect. Psychol. Sci.},
  publisher = {SAGE Publications},
  volume = {11},
  issue = {4},
  pages = {546-573},
  date = {2016-07-29},
  doi = {10.1177/1745691616652873},
  pmid = {27474142},
  issn = {1745-6916,1745-6924},
  abstract = {Good self-control has been linked to adaptive outcomes such as
  better health, cohesive personal relationships, success in the workplace and
  at school, and less susceptibility to crime and addictions. In contrast,
  self-control failure is linked to maladaptive outcomes. Understanding the
  mechanisms by which self-control predicts behavior may assist in promoting
  better regulation and outcomes. A popular approach to understanding
  self-control is the strength or resource depletion model. Self-control is
  conceptualized as a limited resource that becomes depleted after a period of
  exertion resulting in self-control failure. The model has typically been
  tested using a sequential-task experimental paradigm, in which people
  completing an initial self-control task have reduced self-control capacity and
  poorer performance on a subsequent task, a state known as ego depletion
  Although a meta-analysis of ego-depletion experiments found a medium-sized
  effect, subsequent meta-analyses have questioned the size and existence of the
  effect and identified instances of possible bias. The analyses served as a
  catalyst for the current Registered Replication Report of the ego-depletion
  effect. Multiple laboratories (k = 23, total N = 2,141) conducted replications
  of a standardized ego-depletion protocol based on a sequential-task paradigm
  by Sripada et al. Meta-analysis of the studies revealed that the size of the
  ego-depletion effect was small with 95\% confidence intervals (CIs) that
  encompassed zero (d = 0.04, 95\% CI [-0.07, 0.15]. We discuss implications of
  the findings for the ego-depletion effect and the resource depletion model of
  self-control.},
  url = {https://journals.sagepub.com/doi/full/10.1177/1745691616652873},
  urldate = {2023-09-05},
  keywords = {energy model; meta-analysis; resource depletion; self-regulation;
  strength model},
  language = {en}
}

@ARTICLE{Lakens2018-ri,
  title = {Equivalence testing for psychological research: A tutorial},
  author = {Lakens, Daniël and Scheel, Anne M and Isager, Peder M},
  journaltitle = {Adv. Methods Pract. Psychol. Sci.},
  publisher = {SAGE Publications},
  volume = {1},
  issue = {2},
  pages = {259-269},
  date = {2018-06-01},
  doi = {10.1177/2515245918770963},
  issn = {2515-2459,2515-2467},
  abstract = {Psychologists must be able to test both for the presence of an
  effect and for the absence of an effect. In addition to testing against zero,
  researchers can use the two one-sided tests (TOST) procedure to test for
  equivalence and reject the presence of a smallest effect size of interest
  (SESOI). The TOST procedure can be used to determine if an observed effect is
  surprisingly small, given that a true effect at least as extreme as the SESOI
  exists. We explain a range of approaches to determine the SESOI in
  psychological science and provide detailed examples of how equivalence tests
  should be performed and reported. Equivalence tests are an important extension
  of the statistical tools psychologists currently use and enable researchers to
  falsify predictions about the presence, and declare the absence, of meaningful
  effects.},
  url = {https://journals.sagepub.com/doi/full/10.1177/2515245918770963},
  urldate = {2023-08-03},
  language = {en}
}

@ARTICLE{Nosek2020-vh,
  title = {What is replication?},
  author = {Nosek, Brian A and Errington, Timothy M},
  journaltitle = {PLoS Biol.},
  publisher = {Public Library of Science (PLoS)},
  volume = {18},
  issue = {3},
  pages = {e3000691},
  date = {2020-03-27},
  doi = {10.1371/journal.pbio.3000691},
  pmc = {PMC7100931},
  pmid = {32218571},
  issn = {1545-7885,1544-9173},
  abstract = {Credibility of scientific claims is established with evidence for
  their replicability using new data. According to common understanding,
  replication is repeating a study's procedure and observing whether the prior
  finding recurs. This definition is intuitive, easy to apply, and incorrect. We
  propose that replication is a study for which any outcome would be considered
  diagnostic evidence about a claim from prior research. This definition reduces
  emphasis on operational characteristics of the study and increases emphasis on
  the interpretation of possible outcomes. The purpose of replication is to
  advance theory by confronting existing understanding with new evidence.
  Ironically, the value of replication may be strongest when existing
  understanding is weakest. Successful replication provides evidence of
  generalizability across the conditions that inevitably differ from the
  original study; Unsuccessful replication indicates that the reliability of the
  finding may be more constrained than recognized previously. Defining
  replication as a confrontation of current theoretical expectations clarifies
  its important, exciting, and generative role in scientific progress.},
  url = {https://journals.plos.org/plosbiology/article/file?id=10.1371/journal.pbio.3000691&type=printable},
  urldate = {2023-07-31},
  language = {en}
}

@ARTICLE{Spence2016-tz,
  title = {Prediction interval: What to expect when you're expecting … A
  replication},
  author = {Spence, Jeffrey R and Stanley, David J},
  journaltitle = {PLoS One},
  publisher = {Public Library of Science (PLoS)},
  volume = {11},
  issue = {9},
  pages = {e0162874},
  date = {2016-09-19},
  doi = {10.1371/journal.pone.0162874},
  pmc = {PMC5028066},
  pmid = {27644090},
  issn = {1932-6203},
  abstract = {A challenge when interpreting replications is determining whether
  the results of a replication "successfully" replicate the original study.
  Looking for consistency between two studies is challenging because individual
  studies are susceptible to many sources of error that can cause study results
  to deviate from each other and the population effect in unpredictable
  directions and magnitudes. In the current paper, we derive methods to compute
  a prediction interval, a range of results that can be expected in a
  replication due to chance (i.e., sampling error), for means and commonly used
  indexes of effect size: correlations and d-values. The prediction interval is
  calculable based on objective study characteristics (i.e., effect size of the
  original study and sample sizes of the original study and planned replication)
  even when sample sizes across studies are unequal. The prediction interval
  provides an a priori method for assessing if the difference between an
  original and replication result is consistent with what can be expected due to
  sample error alone. We provide open-source software tools that allow
  researchers, reviewers, replicators, and editors to easily calculate
  prediction intervals.},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5028066/},
  urldate = {2023-08-03},
  language = {en}
}

@ARTICLE{Hedges2019-ar,
  title = {More Than One Replication Study Is Needed for Unambiguous Tests of
  Replication},
  author = {Hedges, Larry V and Schauer, Jacob M},
  journaltitle = {J. Educ. Behav. Stat.},
  publisher = {American Educational Research Association},
  volume = {44},
  issue = {5},
  pages = {543-570},
  date = {2019-10-01},
  doi = {10.3102/1076998619852953},
  issn = {1076-9986},
  abstract = {The problem of assessing whether experimental results can be
  replicated is becoming increasingly important in many areas of science. It is
  often assumed that assessing replication is straightforward: All one needs to
  do is repeat the study and see whether the results of the original and
  replication studies agree. This article shows that the statistical test for
  whether two studies obtain the same effect is smaller than the power of either
  study to detect an effect in the first place. Thus, unless the original study
  and the replication study have unusually high power (e.g., power of 98\%), a
  single replication study will not have adequate sensitivity to provide an
  unambiguous evaluation of replication.},
  url = {https://doi.org/10.3102/1076998619852953},
  urldate = {2023-09-01},
  keywords = {replication-methods},
  language = {en}
}

@ARTICLE{Rouder2009-jh,
  title = {Bayesian t tests for accepting and rejecting the null hypothesis},
  author = {Rouder, Jeffrey N and Speckman, Paul L and Sun, Dongchu and Morey,
  Richard D and Iverson, Geoffrey},
  journaltitle = {Psychon. Bull. Rev.},
  volume = {16},
  issue = {2},
  pages = {225-237},
  date = {2009-04},
  doi = {10.3758/PBR.16.2.225},
  pmid = {19293088},
  issn = {1069-9384},
  abstract = {Progress in science often comes from discovering invariances in
  relationships among variables; these invariances often correspond to null
  hypotheses. As is commonly known, it is not possible to state evidence for the
  null hypothesis in conventional significance testing. Here we highlight a
  Bayes factor alternative to the conventional t test that will allow
  researchers to express preference for either the null hypothesis or the
  alternative. The Bayes factor has a natural and straightforward
  interpretation, is based on reasonable assumptions, and has better properties
  than other methods of inference that have been advocated in the psychological
  literature. To facilitate use of the Bayes factor, we provide an easy-to-use,
  Web-based program that performs the necessary calculations.},
  url = {http://dx.doi.org/10.3758/PBR.16.2.225},
  keywords = {Bayesian Statistics},
  language = {en}
}

@BOOK{Jeffreys1973-bp,
  title = {Scientific Inference},
  author = {Jeffreys, Harold},
  publisher = {Cambridge University Press},
  location = {Cambridge, England},
  date = {1973-12-13},
  pagetotal = {244},
  isbn = {9780521084468},
  language = {en}
}

